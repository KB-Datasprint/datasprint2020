{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting terms\n",
    "\n",
    "In this notebook we will look at different ways of counting words. We begin by simply counting how many times a specific term appears in a text. We proceed to calculate relative frequencies, which lets us compare occurences of a term across different texts. Finally, we cover the term frequency - inverse document frequency analysis, which scores terms on how characteristic they are to a specific text.\n",
    "\n",
    "We also start writing our own functions and introduce various new datatypes.\n",
    "\n",
    "## Setup\n",
    "\n",
    "We start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a data source from the [Mediestream API](https://datasprint2020.kb.dk/framework/) and load it to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_1844 = 'https://loar.kb.dk/rest/bitstreams/d3026735-7126-42ce-8fcc-65e9cf04046b/retrieve'\n",
    "\n",
    "df = pd.read_csv(api_1844, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the data are loaded correctly we inspect the DataFrame with `df.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple counting\n",
    "\n",
    "The pandas library has various built-in method we can use to describe our data.\n",
    "\n",
    "Below we count all occurences of a specific term in the dataset.\n",
    "\n",
    "First, we save our search term to a variable, which will make it easier to swap between search terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = 'Slesvig'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newspaper text data are located in the `fulltext_org` column. We access this column of our DateFrame with bracket notation and use the string method `count` to count the occurence of our search term in each row of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fulltext_org'].str.count(search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get the total number of occurences across all rows, we apply the `sum` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fulltext_org'].str.count(search_term).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to investigate a specific newspaper, we can filter the data before counting occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['editionId'].str.contains('faedrelandet1834')]['fulltext_org'].str.count(search_term).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax is a bit convoluted but the expression `df['editionId'].str.contains('faedrelandet1834')` checks whether the name of the target newspaper is present in the `editionId` column. We pass the output of the expression to our DataFrame `df`, which only returns the rows where the expression evaluates to `True`. Finally, we apply the `count` and `sum` methods as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating relative frequencies\n",
    "\n",
    "Relative frequency is a method for comparing occurences of keywords across a number of differently sized texts. \n",
    "\n",
    "Relative frequency is calculated by dividing the number of occurences of a word with the total number of words in the text:\n",
    "\n",
    "$$f_i = \\frac{n_i}{N}$$\n",
    "\n",
    "Translated into code we can do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID of target newspaper.\n",
    "paper = 'faedrelandet1834'\n",
    "\n",
    "# The number of occurences of our search term.\n",
    "hits = df[df['editionId'].str.contains(paper)]['fulltext_org'].str.count(search_term).sum()\n",
    "\n",
    "# The total number of words.\n",
    "total = df[df['editionId'].str.contains(paper)]['fulltext_org'].str.split().str.len().sum()\n",
    "\n",
    "# Relative frequency calculation\n",
    "rf = hits / total\n",
    "\n",
    "print(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get a very small number, which in itselt does not say alot. However, if we repeat the process with a different newspaper title we have a measure for comparing the two without having to worry about the individual sizes of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID of target newspaper.\n",
    "paper1 = 'berlingsketidende'\n",
    "\n",
    "# The number of occurences of our search term.\n",
    "hits1 = df[df['editionId'].str.contains(paper1)]['fulltext_org'].str.count(search_term).sum()\n",
    "\n",
    "# The total number of words.\n",
    "total1 = df[df['editionId'].str.contains(paper1)]['fulltext_org'].str.split().str.len().sum()\n",
    "\n",
    "# Relative frequency calculation\n",
    "rf1 = hits1 / total1\n",
    "\n",
    "print(rf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we also calculate the relative frequency of our search term across the entire dataset for an average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of occurences of our search term.\n",
    "full_hits = df['fulltext_org'].str.count(search_term).sum()\n",
    "\n",
    "# The total number of words.\n",
    "full_total = df['fulltext_org'].str.split().str.len().sum()\n",
    "\n",
    "full_rf = full_hits / full_total\n",
    "\n",
    "print(full_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualise our findings with the `matplotlib` library (`plt`).\n",
    "\n",
    "We create two lists; one with our newspaper titles (`labels`) and one with our relative frequencies (`values`). We then pass the two lists to the `bar` function, which returns a nice bar plot that illustrates the difference between the newspapers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = ['FÃ¦drelandet', 'Berlingske Tidende', 'Total']\n",
    "\n",
    "values = [rf, rf1, full_rf]\n",
    "\n",
    "plt.bar(labels, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing code with functions\n",
    "\n",
    "In the examples above we reused a lot of the same code. Whenever we repeat code it might be useful to write a function instead, which let's us reuse code over and over.\n",
    "\n",
    "Below we refactor our relative frequency calculations into a function. The function takes as arguments a search term, a list of one or more newpaper IDs and a data source (`df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_frequencies(term, newspaper_list, data, include_total=True, return_data=False):\n",
    "    \"\"\"Calculate relative frequencies for words in a text collection.\n",
    "    \n",
    "    The function calculates the relative frequency of a word based on \n",
    "    their occurence in specified newspapers.\n",
    "    \n",
    "    At least one newspaper ID must be provided in a list and the source of text data\n",
    "    should a DataFrame with data loaded from Mediestream.\n",
    "    \n",
    "    By default, the relative frequency across all newspapers is included.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare list for relative frequency data\n",
    "    values = []\n",
    "    \n",
    "    # For each newspaper title we find the number of matches and the total word count.\n",
    "    for paper in newspaper_list:\n",
    "\n",
    "        hits = data[data['editionId'].str.contains(paper)]['fulltext_org'].str.count(term).sum()\n",
    "\n",
    "        total = data[data['editionId'].str.contains(paper)]['fulltext_org'].str.split().str.len().sum()\n",
    "        \n",
    "        # To avoid division-by-zero errors, we only calculate the relative frequency if there is at least 1 hit.\n",
    "        # If there are no hits, we add zero to the list of relative frequencies.\n",
    "        if hits > 0:\n",
    "            values.append(hits / total)\n",
    "        else:\n",
    "            values.append(0)\n",
    "    \n",
    "    if include_total:\n",
    "        newspaper_list.append('Total')\n",
    "        \n",
    "        values.append(data['fulltext_org'].str.count(term).sum() / data['fulltext_org'].str.split().str.len().sum())\n",
    "    \n",
    "    # Draw the plot\n",
    "    plt.bar(newspaper_list, values)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend([term])\n",
    "    plt.show()\n",
    "    \n",
    "    # If we need to work with the relative frequencies outside the function,\n",
    "    # we can set the return_data parameter to True\n",
    "    if return_data:\n",
    "        return values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate relative frequencies across any number of newspapers in a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "paper_list = ['berlingsketidende', 'faedrelandet1834', 'dannevirke1838', 'ribestiftstidende']\n",
    "\n",
    "get_relative_frequencies('dansk', paper_list, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of counting terms is the Term Frequency - Inverse Document Frequency (tf-idf) analysis. The tf-idf method is useful for identifying terms which are significant for a specific document across a number of documents. The tf part identifies the most common words for a single document, while the idf part penalises the words if they occur in other documents as well. That means that common words such as _the_ and _a_ receive a high tf score but the score is negated by idf because the words most likely occur in all documents.\n",
    "\n",
    "The result of the tf-idf calculation is a list of the most unique words for the specific document along with a score of how unique they are.\n",
    "\n",
    "The tf-idf is calculated by multiplying tf and idf. This means, we first need to calculate these two statistics.\n",
    "\n",
    "Before we start, we need to rearrange our data into a more suitable format. Our starting point will be a text string for each newspaper we want to include in the analysis. We extract all text data from two newspapers and save them to two text strings (`text_a` and `text_b`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspaper_a = 'faedrelandet1834'\n",
    "newspaper_b = 'dannevirke1838'\n",
    "\n",
    "text_a = ' '.join(df[df['editionId'].str.contains(newspaper_a)]['fulltext_org'])\n",
    "text_b = ' '.join(df[df['editionId'].str.contains(newspaper_b)]['fulltext_org'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we want to use texts saved as text files, we can load them into text strings with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to file locations\n",
    "path_to_text_a = '../../data/faedrelandet1834_fulltext.txt'\n",
    "path_to_text_b = '../../data/dannevirke1838_fulltext.txt'\n",
    "\n",
    "with open(path_to_text_a) as f:\n",
    "    text_a = f.read()\n",
    "\n",
    "with open(path_to_text_b) as f:\n",
    "    text_a = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the strings into lists\n",
    "Once we have our texts loaded into strings, we split the strings into lists of words. This kind of list is often refered to as a bag of words. The bag of words method is useful for analyses where word order is irrelevant such as tf-idf. This means that we could randomise the word order and make the text unintelligble to humans without changing the outcome of the analysis. This is especially useful when working with sensitive data.\n",
    "\n",
    "Below we use the `split()` method which by default will split our string on whitespace ie. space, tab and line-break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_a = text_a.split()\n",
    "bag_of_words_b = text_b.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __OPTIONAL__: NLTK tokenization\n",
    "If we want to split the our strings into words in a more sophisticated way, we can use a dedicated tokenizer.\n",
    "\n",
    "Apart from splitting on whitespace, NLTK's `word_tokenize` function takes care of punctuation and other neat stuff. The trade-off is that it is much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "bag_of_words_a = word_tokenize(text_a)\n",
    "bag_of_words_b = word_tokenize(text_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF calculations\n",
    "Now, our data are prepared for the analysis. Next, we need to count all the words in our two texts. We use the `set` datatype to create a list of unique words. Sets are like lists except that all elements in a set must be unique. That means that when we convert a list into a set, all duplicates are removed.\n",
    "\n",
    "To get all the unique words across the two texts, we join our two bags of words and convert them to a set. Now that we have changed the datatype, no duplicates are allowed and we get a list (technically not a list but a set) of all unique words across the two texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words  = set(bag_of_words_a + bag_of_words_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know all the unique words of the texts, we just need to count them.\n",
    "\n",
    "To keep track of our word counts, we introduce a new datatype: the dictionary (`dict`). Dictionaries are useful when we want to store data in a way that allows us to access specific parts of the data easily.\n",
    "\n",
    "#### OPTIONAL: More about dictionaries\n",
    "Dictionaries are related to lists, because they can both store multiple values. However, instead of numbers, the index of a dictionary are accessed by using a _key_.\n",
    "\n",
    "An example: I have a a dictionary (`days_per_month`) and I want to save the number of days in each month in the dictionary. I use the names of the months as my keys and the number of days are my values. To add the values to the dictionary I assign the value at the correct index; `days_per_month['january'] = 31` and repeat for the remaining months. Now I can retrieve the values by passing a specific key to the dictionary, eg. `days_per_month['november']` will return the value `30`. If I pass key that has not been assigned a value, I will get an error.\n",
    "\n",
    "Dictionaries are initiated with either curly brackets (`{}`) or the `dict()` command. Our previous dictionary could have been initiated with either `days_per_month = {}` or `days_per_month = dict()`.\n",
    "\n",
    "Dictionaries doesn't have to be empty from the beginning; they can be initated with data. The syntax requires each key and value to be separated by a colon and each key-value pair to be separated by a comma. Our previous dictionary could be initiated with data in this way:\n",
    "\n",
    "`days_per_month = {'january': 31,\n",
    "                   'february': 28,\n",
    "                   'march': 31,\n",
    "                   'april': 30,\n",
    "                   'may': 31,\n",
    "                   'june': 30,\n",
    "                   'july': 31,\n",
    "                   'august': 31,\n",
    "                   'september': 30,\n",
    "                   'october': 31,\n",
    "                   'november': 30,\n",
    "                   'december': 31\n",
    "                  }`\n",
    "\n",
    "__Enough about dictionaries - back to our analysis.__\n",
    "\n",
    "We use the `dict` command with the `fromkeys` method to create a dictionary which has all the words from `unique_words` as keys with an initial value of 0 each.\n",
    "\n",
    "Then we use a `for` loop to iterate over every word in our first bag of words. Each time, we use the word as key in our dictionary of all unique words (`num_of_words_a`) and increment the value by 1. Once the loop has finished our dictionary contains the word counts for all words in our first text.\n",
    "\n",
    "We repeat the process for our second bag of words and save the word counts in another dictionary (`num_of_words_b`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words_a = dict.fromkeys(unique_words, 0)\n",
    "\n",
    "for word in bag_of_words_a:\n",
    "    num_of_words_a[word] += 1\n",
    "\n",
    "num_of_words_b = dict.fromkeys(unique_words, 0)\n",
    "\n",
    "for word in bag_of_words_b:\n",
    "    num_of_words_b[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Quick function for sorting dictionaries by count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term frequency\n",
    "Similar to relative frequency, term frequency is a measure of how many times a term occurs in a document relative to the length of the document.\n",
    "\n",
    "We define a function which lets us calculate the term frequency for all words in our word count dictionary at the same time.\n",
    "\n",
    "The function needs a word count dictionary and a bag of words. After some initial setup, the function loops over all the items in the word count dictionary, calculates tf by dividing the word count for the specific word with the total number of words in the bag of words and saves the value to a new dictionary. Finally, the new dictionary with the tf value for each word is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(num_of_words, bag_of_words):\n",
    "    # Empty dictionary for results\n",
    "    tf_dict = dict()\n",
    "    \n",
    "    # The number of words in the bag of words\n",
    "    bag_of_words_count = len(bag_of_words)\n",
    "    \n",
    "    # Calculate term frequency and add to new dictionary\n",
    "    for word, count in num_of_words.items():\n",
    "        tf_dict[word] = count / bag_of_words_count\n",
    "    \n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequencies are calculated for both texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_a = compute_tf(num_of_words_a, bag_of_words_a)\n",
    "tf_b = compute_tf(num_of_words_b, bag_of_words_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse document frequency\n",
    "Next, we need to calculate the inverse document frequency. Again we define a function that lets us process our entire texts at once.\n",
    "\n",
    "This function is a bit more complex because we use nested loops (i.e. loops within loops) and a bit more math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The math module is needed to calculate logarithm\n",
    "import math\n",
    "\n",
    "def compute_idf(documents):\n",
    "    # Number of texts\n",
    "    N = len(documents)\n",
    "    \n",
    "    # Dictionary with all unique words and initial values of 0.\n",
    "    idf_dict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    \n",
    "    # By looping through each word of each text,\n",
    "    # idf_dict initially contains the number texts\n",
    "    # each specific word appears in.\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idf_dict[word] += 1\n",
    "    \n",
    "    # idf is then calculated for each word\n",
    "    # and the inital values are replaced.\n",
    "    for word, val in idf_dict.items():\n",
    "            idf_dict[word] = math.log(N / val)\n",
    "    \n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word counts for the two texts are passed to the function as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = compute_idf([num_of_words_a, num_of_words_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "Now that we have the inverse document frequency for each unique word we can finally calculate the tf-idf by multiplying the tf values of each text with the idf values. Again, we use a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(tf_dict, idf):\n",
    "    #Empty dictionary for results\n",
    "    tf_idf = dict()\n",
    "    \n",
    "    # tf-idf calculation for each word\n",
    "    for word, val in tf_dict.items():\n",
    "        tf_idf[word] = val * idf[word]\n",
    "    \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can get the tf-idf for our two texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_a = compute_tf_idf(tf_a, idfs)\n",
    "tf_idf_b = compute_tf_idf(tf_b, idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the DataFrame\n",
    "Now that our analysis is done, we return to pandas and convert our results into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df = pd.DataFrame([tf_idf_a, tf_idf_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the words most unique to the first text, we sort the values of the first row and return the first 10 elements with `head(10)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_a = tf_idf_df.loc[0].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the words which, according to the tf-idf analysis, are the most significant to this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(top10_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we can visualise the results with a simple bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bar plot with labels and values.\n",
    "plt.bar(top10_a.index, top10_a)\n",
    "\n",
    "# Rotate labels to avoid overlap.\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the results for the second text, we repeat the process with the second row of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top10_b = tf_idf_df.loc[1].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot with labels and values.\n",
    "plt.bar(top10_b.index, top10_b)\n",
    "\n",
    "# Rotate labels to avoid overlap.\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This concludes our counting notebook. We have covered different ways of counting terms at different levels. From simple word counts over relative frequency to tf-idf.\n",
    "\n",
    "Each analysis can be modified and improved in various ways. The tf-idf in particular would benefit from using more texts for a more robust result. All analyses could also be improved by processing and cleaning the data first; for instance by lower casing all words and removing punctuation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
