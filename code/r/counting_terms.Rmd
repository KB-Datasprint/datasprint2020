---
title: "Relative frequencies"
author: "Max Odsbjerg Pedersen"
date: "10/20/2020"
output: 
    html_document:
      df_print: paged
      toc: true
      toc_depth: 2
      toc_float: true
---

In this rmarkdown we will look at different ways of counting words.

We start by loading the relavant libraries:

```{r, message=FALSE}
library(tidyverse)
library(tidytext)
```

We load the data from the LOAR-API that contains the newspaper data from the year 1849:

```{r, message=FALSE}
artikler_1848 <- read_delim("https://loar.kb.dk/rest/bitstreams/f2437eca-c354-46c0-aee2-022279050ce3/retrieve",
  delim = ",",
  escape_backslash = TRUE,
  escape_double = FALSE)
```
For other years see under "data" on the following page: https://datasprint2020.kb.dk/framework/

# Cleaning the data

Extracting the title of the newspaper:

```{r, message=FALSE, warning=FALSE}
artikler_1848 <- artikler_1848 %>% 
  separate(editionId, sep=" ", into = c("title"), remove = FALSE)
```

To ensure that the data are loaded and cleaned correctly we inspect the DataFrame with `head()`.

```{r}
artikler_1848 %>% 
  head()
```

# Counting words

The next step is to put each word on it's own row in order to count how many times a specific word appears

```{r}
artikler_1848 %>% 
  unnest_tokens(word, fulltext_org) -> artikler_1848_tidy
```

```{r}
artikler_1848_tidy %>%
  filter(word == "slesvig") %>% 
  count(word)
```

This count is how many times "slesvig" appears in all the newspapers in 1849. But what if we only want to see it in one specific newspaper?

```{r}
artikler_1848_tidy %>%
  filter(str_detect(editionId,"faedrelandet1834")) %>% 
  filter(word == "slesvig") %>% 
  count(word)
```

## Calculating relative frequencies

Relative frequency is a method of comparing occurences of keywords across a number of differently sized texts.

Relative frequency is calculated by dividing the number of occurences of a word with the total number of words in the text:

$$f_i = \frac{n_i}{N}$$

Translated into code we can do the following:

```{r}
hits <- artikler_1848_tidy %>%
  filter(str_detect(editionId,"faedrelandet1834")) %>% 
  filter(word == "slesvig") %>% 
  count(word)

total <- artikler_1848_tidy %>% 
  filter(str_detect(editionId,"faedrelandet1834")) %>% 
  count(word) %>% 
  summarise(sum(n))
```

With the two numbers at hand we can now calculate the relative frequency:

```{r}
hits$n/total$`sum(n)`
```

This is a very cumbersome and just to serve as an example for us. Luckily R and the `tidytext`package can calculate frequencies for all the words with just one function!

```{r}
artikler_1848_tidy %>%
  count(word, title) %>% 
  bind_tf_idf(word, title, n) -> artikler_1848_tf_idf
```

```{r}
artikler_1848_tf_idf %>% 
  filter(word == "slesvig")
```

Let's visualise the relative frequencies to get a better overview over which newspaper was occupied with "slesvig":

```{r}
artikler_1848_tf_idf %>% 
  filter(word == "slesvig") %>% 
  arrange(desc(tf)) %>% 
  ggplot(aes(reorder(title, tf), y = tf)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Newspaper",
      y = "Relative frequency",
      title = "Relative frequencies for 'slesvig' in 1848 Danish newspapers")
```

# Seeing how newspapers differs - term frequency - inversed document frequency

We just calculated all the word's frequencies to compare the use of the word "Slesvig". This is useful if we before hand have our word of interest. In this analysis we wont try to find specific words but instead use a method that compares the newspapers to eachother and find the words that are specific to each newspaper. This method is called term frequency - inversed document frequency.\
We already worked with and calculated term frequencies so if we take a look at the terms with the highest frequency:

```{r}
artikler_1848_tf_idf %>% 
  arrange(desc(tf)) %>% 
  select(word, title, tf)
```

Now we will find the words that appear most commonly per newspaper in the data from 1848

```{r chunk 5 - word counts pr. chapter}
 artikler_1848_tidy %>% 
  count(title, word, sort = TRUE)
```

Not surprisingly, particles are the most common words we find. This is not particularly interesting for us in this enquiry, as we want to see which words are specific to the individual newspapers. The particles will appear in all newspapers and have high frequencies. We now need a measurement to punish words such as "og" and "den", which is expected to appear within all the newspapers in our dataset (except perhaps the ones from St. Croix and St. Thomas, which are in English). This is where the inversed document frequnecy enters the scene:

$$\textrm{idf}(term)=\ln(\frac{n}{N})$$ n is the totalt number of documents (chapters, in our example) and N is the number of chapters in which the word occurs.

here we calculate for the word "og".

$$\textrm{idf}(og)=\ln(\frac{16}{16})=0$$ Thus we punish words that occur with great frequency in all newspapers or many newspapers. Words that occur in all the chapters cannot really tell us much about a particular chapter. Those words will have an idf of 0 resulting in a tf\_idf value that is also 0, because this is defined by multiplying tf with idf.

Luckily, R can easily calculate tf and tf\_idf for all the words by using the bind\_tf\_idf function. This is the same function as we used earlier to find the frequency for Slesvig, so actually we already have the term frequency and inversed document frequency calculated. The same function also multiplies the two numbers together with is our measurement for how specific a word is to a given newspaper. Lets examine the data frame before where we arrange on the highest tf\_idf-value, which makes the most important word to the newspapers:

```{r chunk 9 - calculating tf and idf}
artikler_1848_tf_idf %>% 
  arrange(desc(tf_idf))
```

Turns out English words from the two newspaper from the Danish West Indies is represented a fair bit in the top of this list. These words will have a high idf because they only are present in two out sixteen newspapers. This method thus works the best when you are comparing withing a single language.

Lets once more look at the highest tf\_idf-values, but this time we are filtering out the two English newspapers:

```{r}
artikler_1848_tf_idf %>% 
  filter(title != "stcroixavisdvi") %>%
  filter(title != "sanctthomaetidendedvi") %>% 
  arrange(desc(tf_idf))
```

Next step is to visualise our result to get a better overview!

# Visualisation

Many people who have tip their toes in the text mining/data mining sea will have seen wordclouds showing the most used words in a text. I this visualisation we are going to create a wordcloud for each newspaper showing the words with the highest tf\_idf from that newspaper. By doing so we will get a nice overview of what words are specific and important to each newspaper. Remember that words which appear alot acros the chapters will not show here.

```{r}
library(ggwordcloud)
```

```{r}
artikler_1848_tf_idf <- artikler_1848_tf_idf %>% 
  group_by(title) %>% 
  mutate(rel_placering = (tf_idf - min(tf_idf, na.rm = T))/(max(tf_idf, na.rm = T) - min(tf_idf, na.rm = T))) %>% 
  ungroup()
```

```{r chunk 11 - visualisation}
artikler_1848_tf_idf %>%
  filter(title != "stcroixavisdvi") %>%
  filter(title != "sanctthomaetidendedvi") %>% 
  arrange(desc(rel_placering)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(title) %>% 
  top_n(8) %>% 
  ungroup %>%
  ggplot(aes(label = word, size = rel_placering, color = rel_placering)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 15) +
  theme_minimal() +
  facet_wrap(~title, ncol = 3, scales = "free") +
  scale_color_gradient(low = "darkgoldenrod2", high = "darkgoldenrod4") +
  labs(
      title = "Newspapers 1848: most important words for each newspaper",
       subtitle = "Importance determined by term frequency (tf) - inversed document frequency(idf)",
      caption = "Data from loar.kb.dk")

```

Because the space for visualisation is constricted in this .Rmd format we have to save the result as a pdf, where we define a larger canvas. Run the last code, chunk 12, and look in the files pane to the right. In the folder "doc" you should get a file called "newspapers\_1848\_tfidf.pdf". This is readable.

```{r chunk 12 - saving visualisation as pdf}
ggsave("newspapers_1848_tfidf.pdf", width = 65, height = 35, units = "cm")
```

# Seeing how specific newspapers differs - term frequency - inversed document frequency

Before we looked at how all the newspapers differed from eachother in this case we want to examine how the Copenhagen based newspaper "Fædrelandet" differs from the Schleswig-based "Dannevirke".

The first step is to extract these two newspapers out:

```{r}
artikler_1848_tidy %>% 
  filter(title %in% c("faedrelandet1834", "dannevirke1838")) -> FD_tidy
```

Next we calculate the tf and idf and tf-idf for these two:

```{r}
FD_tidy %>%
  count(word, title) %>% 
  bind_tf_idf(word, title, n) -> FD_tf_idf
```

This time we jump straigt to the visualisation of the highest tf-idf values:

```{r}
FD_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(title) %>% 
  top_n(12) %>% 
  ungroup %>%
  ggplot(aes(label = word, size = tf_idf, color = tf_idf)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 8) +
  theme_minimal() +
  facet_wrap(~title, ncol = 3, scales = "free") +
  scale_color_gradient(low = "darkred", high = "red") +
  labs(
      title = "1848: Fædrelandet and Dannevirke: most important words for each newspaper",
       subtitle = "Importance determined by term frequency (tf) - inversed document frequency(idf)",
      caption = "Data from loar.kb.dk")

```
```{r}
ggsave("FD_tf_idf_cloud.png", width = 18, height = 11, units = "cm")
```

# Examining the words in context
```{r}
artikler_1848 %>%
  filter(title == "dannevirke1838") %>% 
  filter(str_detect(fulltext_org, "fjendtlige"))
```
The actual context on the scanned document can be seen in Mediestream by searching in Mediestream for the uuid found under `recordID` and removing the last segment part. Here we use the first row from the above dataframe as example: 

`recordID`:  
doms_newspaperCollection:uuid:59587af0-e660-4a4a-bf39-3123affa31a2-segment_4

`uuid`without segment-X-end:  
uuid:59587af0-e660-4a4a-bf39-3123affa31a2

Search term for Mediestream:  
pageUUID:"doms_aviser_page:uuid:59587af0-e660-4a4a-bf39-3123affa31a2"